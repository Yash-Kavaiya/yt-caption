name: YouTube Screenshot Capture - Parallel Processing

on:
  push:
    branches:
      - main
      - master
    paths:
      - '**/*.txt'
      - '.github/workflows/youtube-screenshot.yml'
  workflow_dispatch:
    inputs:
      video_url:
        description: 'YouTube video URL (optional)'
        required: false
        type: string
      interval:
        description: 'Screenshot interval in seconds'
        required: false
        default: '5'
        type: string

jobs:
  scan-videos:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      has-videos: ${{ steps.set-matrix.outputs.has-videos }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Scan all .txt files and create matrix
        id: set-matrix
        run: |
          echo "Scanning for video URLs in .txt files..."
          
          # Create JSON array for matrix
          videos_json="["
          first=true
          
          # Find all .txt files (including subdirectories)
          find . -type f -name "*.txt" \
            ! -path "./output/*" \
            ! -path "./.git/*" \
            ! -path "./.*" \
            -print0 | while IFS= read -r -d '' txt_file; do
            
            echo "Processing file: $txt_file"
            
            # Read each line from the file
            while IFS= read -r line || [ -n "$line" ]; do
              # Skip empty lines and comments
              if [[ -z "$line" ]] || [[ "$line" =~ ^[[:space:]]*# ]]; then
                continue
              fi
              
              # Extract URL and interval
              URL=$(echo "$line" | cut -d',' -f1 | xargs)
              INTERVAL=$(echo "$line" | cut -d',' -f2 | xargs)
              
              # Validate URL
              if [[ ! "$URL" =~ ^https?:// ]]; then
                echo "Skipping invalid URL: $URL"
                continue
              fi
              
              # Default interval
              if [ -z "$INTERVAL" ] || [ "$INTERVAL" = "$URL" ]; then
                INTERVAL=5
              fi
              
              # Add to JSON array
              if [ "$first" = true ]; then
                first=false
              else
                videos_json+=","
              fi
              
              # Escape quotes in URL for JSON
              URL_ESCAPED=$(echo "$URL" | sed 's/"/\\"/g')
              videos_json+="{\"url\":\"$URL_ESCAPED\",\"interval\":$INTERVAL,\"source\":\"$txt_file\"}"
              
            done < "$txt_file"
          done
          
          videos_json+="]"
          
          echo "Generated matrix JSON:"
          echo "$videos_json"
          
          # Check if we have any videos
          video_count=$(echo "$videos_json" | jq '. | length')
          echo "Total videos found: $video_count"
          
          if [ "$video_count" -gt 0 ]; then
            echo "has-videos=true" >> $GITHUB_OUTPUT
            echo "matrix={\"include\":$videos_json}" >> $GITHUB_OUTPUT
          else
            echo "has-videos=false" >> $GITHUB_OUTPUT
            echo "matrix={\"include\":[]}" >> $GITHUB_OUTPUT
          fi

  process-video:
    needs: scan-videos
    if: needs.scan-videos.outputs.has-videos == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      max-parallel: 5  # Process 5 videos at a time
      fail-fast: false  # Continue even if one fails
      matrix: ${{fromJson(needs.scan-videos.outputs.matrix)}}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y ffmpeg
          pip install --upgrade pip
          pip install yt-dlp Pillow numpy

      - name: Create processing script
        run: |
          cat > youtube_screenshots.py << 'EOF'
          #!/usr/bin/env python3
          import os, sys, re, subprocess, argparse, tempfile, shutil, json, hashlib, textwrap
          from pathlib import Path
          from PIL import Image
          from datetime import timedelta
          
          def sanitize_filename(filename, max_length=100):
              invalid_chars = '<>:"/\\|?*'
              for char in invalid_chars:
                  filename = filename.replace(char, '')
              filename = filename.strip('. ').replace(' ', '_')
              return filename[:max_length] if len(filename) > max_length else filename
          
          def format_time(seconds):
              return str(timedelta(seconds=int(seconds)))
          
          def get_video_info(url):
              try:
                  cmd = ['yt-dlp', '--dump-json', '--no-playlist', url]
                  result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
                  info = json.loads(result.stdout)
                  return {
                      'title': info.get('title', 'untitled'),
                      'duration': info.get('duration', 0),
                      'uploader': info.get('uploader', 'Unknown'),
                      'view_count': info.get('view_count', 0),
                      'subtitles_available': bool(info.get('subtitles', {}) or info.get('automatic_captions', {}))
                  }
              except Exception as e:
                  print(f"Error getting video info: {e}")
                  return None
          
          def download_video_and_transcript(url, video_path, transcript_path):
              try:
                  format_options = [
                      'bestvideo[height>=1080][ext=mp4]+bestaudio[ext=m4a]/best[height>=1080][ext=mp4]',
                      'bestvideo[height>=720][ext=mp4]+bestaudio[ext=m4a]/best[height>=720][ext=mp4]',
                      'best[ext=mp4]/best'
                  ]
                  cmd = ['yt-dlp', '-f', '/'.join(format_options), '--no-playlist',
                         '--merge-output-format', 'mp4', '-o', video_path,
                         '--write-auto-subs', '--write-subs', '--sub-lang', 'en',
                         '--convert-subs', 'srt', url]
                  print("Downloading video...")
                  subprocess.run(cmd, check=True)
                  video_dir = os.path.dirname(video_path)
                  video_base = os.path.splitext(os.path.basename(video_path))[0]
                  subtitle_patterns = [f"{video_base}.en.srt", f"{video_base}.en.vtt",
                                     f"{video_base}.srt", f"{video_base}.vtt"]
                  transcript_found = False
                  for pattern in subtitle_patterns:
                      potential_file = os.path.join(video_dir, pattern)
                      if os.path.exists(potential_file):
                          convert_srt_to_text(potential_file, transcript_path)
                          transcript_found = True
                          break
                  return True, transcript_found
              except Exception as e:
                  print(f"Error downloading video: {e}")
                  return False, False
          
          def convert_srt_to_text(srt_file, text_file):
              try:
                  with open(srt_file, 'r', encoding='utf-8', errors='ignore') as f:
                      lines = f.readlines()
                  transcript_lines = []
                  current_text = []
                  for line in lines:
                      line = line.strip()
                      if line and not line.isdigit() and '-->' not in line:
                          line = re.sub('<[^<]+?>', '', line)
                          current_text.append(line)
                      elif not line and current_text:
                          transcript_lines.append(' '.join(current_text))
                          current_text = []
                  if current_text:
                      transcript_lines.append(' '.join(current_text))
                  with open(text_file, 'w', encoding='utf-8') as f:
                      f.write("VIDEO TRANSCRIPT\n" + "=" * 50 + "\n\n")
                      full_text = ' '.join(transcript_lines)
                      full_text = re.sub(r'\s+', ' ', full_text)
                      wrapped_text = textwrap.fill(full_text, width=80)
                      f.write(wrapped_text)
                  return True
              except Exception as e:
                  print(f"Error converting transcript: {e}")
                  return False
          
          def extract_high_quality_screenshots(video_path, output_dir, interval, title_prefix, quality='highest'):
              try:
                  os.makedirs(output_dir, exist_ok=True)
                  cmd = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',
                         '-of', 'default=noprint_wrappers=1:nokey=1', video_path]
                  result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                  duration = float(result.stdout.strip())
                  print(f"Video duration: {duration:.1f} seconds")
                  print(f"Extracting screenshots every {interval} seconds...")
                  screenshots_taken = 0
                  current_time = 0
                  screenshot_files = []
                  while current_time <= duration:
                      time_str = f"{int(current_time):04d}s"
                      if quality == 'highest':
                          output_file = os.path.join(output_dir, f"{title_prefix}_{time_str}.png")
                          cmd = ['ffmpeg', '-ss', str(current_time), '-i', video_path,
                                 '-vframes', '1', '-vf', 'scale=iw:ih', '-y', output_file]
                      else:
                          output_file = os.path.join(output_dir, f"{title_prefix}_{time_str}.jpg")
                          cmd = ['ffmpeg', '-ss', str(current_time), '-i', video_path,
                                 '-vframes', '1', '-q:v', '1', '-y', output_file]
                      subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
                      screenshots_taken += 1
                      screenshot_files.append(output_file)
                      progress = (current_time / duration) * 100
                      print(f"  [{progress:5.1f}%] Screenshot at {format_time(current_time)}")
                      current_time += interval
                  return screenshots_taken, screenshot_files
              except Exception as e:
                  print(f"Error extracting screenshots: {e}")
                  return 0, []
          
          def get_image_hash(image_path):
              try:
                  with open(image_path, 'rb') as f:
                      return hashlib.sha256(f.read()).hexdigest()
              except Exception:
                  return None
          
          def remove_duplicate_screenshots(screenshot_files):
              if len(screenshot_files) <= 1:
                  return 0
              print("\nRemoving duplicate screenshots...")
              duplicates_removed = 0
              files_to_remove = set()
              processed_hashes = {}
              for current_file in screenshot_files:
                  if current_file in files_to_remove or not os.path.exists(current_file):
                      continue
                  current_hash = get_image_hash(current_file)
                  if not current_hash:
                      continue
                  if current_hash in processed_hashes:
                      files_to_remove.add(current_file)
                  else:
                      processed_hashes[current_hash] = current_file
              for file_path in files_to_remove:
                  try:
                      os.remove(file_path)
                      duplicates_removed += 1
                  except Exception:
                      pass
              return duplicates_removed
          
          def create_hd_pdf(images_dir, pdf_path, dpi=600):
              try:
                  print(f"\nCreating PDF (DPI: {dpi})...")
                  image_files = sorted(list(Path(images_dir).glob('*.jpg')) + list(Path(images_dir).glob('*.png')))
                  if not image_files:
                      return False
                  pdf_images = []
                  for img_path in image_files:
                      try:
                          img = Image.open(img_path)
                          if img.mode != 'RGB':
                              img = img.convert('RGB')
                          max_dimension = 3000
                          if img.width > max_dimension or img.height > max_dimension:
                              img.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)
                          pdf_images.append(img)
                      except Exception:
                          pass
                  if pdf_images:
                      pdf_images[0].save(pdf_path, "PDF", resolution=dpi, quality=95, optimize=True,
                                        save_all=True, append_images=pdf_images[1:] if len(pdf_images) > 1 else [])
                      file_size = os.path.getsize(pdf_path) / (1024 * 1024)
                      print(f"  ✓ PDF created: {file_size:.2f} MB")
                      return True
                  return False
              except Exception as e:
                  print(f"Error creating PDF: {e}")
                  return False
          
          def process_video(url, interval, output_dir='.', quality='highest', pdf_dpi=600):
              print(f"Fetching video information...")
              video_info = get_video_info(url)
              if not video_info:
                  return False
              safe_title = sanitize_filename(video_info['title'])
              print(f"\n{'='*60}")
              print(f"Video: {video_info['title']}")
              print(f"Duration: {format_time(video_info['duration'])}")
              print(f"Uploader: {video_info['uploader']}")
              print(f"{'='*60}\n")
              base_dir = Path(output_dir)
              video_dir = base_dir / safe_title
              images_dir = video_dir / 'images'
              images_dir.mkdir(parents=True, exist_ok=True)
              with tempfile.TemporaryDirectory() as temp_dir:
                  video_path = os.path.join(temp_dir, 'video.mp4')
                  transcript_path = video_dir / f"{safe_title}_transcript.txt"
                  video_success, transcript_success = download_video_and_transcript(url, video_path, transcript_path)
                  if not video_success:
                      print("Error: Failed to download video")
                      return False
                  print(f"\n✓ Video downloaded successfully!")
                  screenshot_count, screenshot_files = extract_high_quality_screenshots(
                      video_path, images_dir, interval, safe_title, quality)
                  if screenshot_count > 0:
                      print(f"\n✓ Extracted {screenshot_count} screenshots")
                      duplicates = remove_duplicate_screenshots(screenshot_files)
                      if duplicates > 0:
                          print(f"✓ Removed {duplicates} duplicates")
                          print(f"  Final: {screenshot_count - duplicates} unique screenshots")
                      pdf_path = video_dir / f"{safe_title}_HD.pdf"
                      create_hd_pdf(images_dir, pdf_path, pdf_dpi)
                      print(f"\n{'='*60}")
                      print(f"✅ COMPLETED SUCCESSFULLY!")
                      print(f"{'='*60}")
                      print(f"📁 Output: {video_dir.absolute()}")
                      print(f"🖼️  Screenshots: {len(screenshot_files) - duplicates}")
                      print(f"📄 PDF: {pdf_path.name}")
                      if transcript_success:
                          print(f"📝 Transcript: {transcript_path.name}")
                      print(f"{'='*60}\n")
                      return True
                  else:
                      print("Error: No screenshots extracted")
                      return False
          
          if __name__ == "__main__":
              parser = argparse.ArgumentParser(description='YouTube Screenshot Tool')
              parser.add_argument('url', help='YouTube video URL')
              parser.add_argument('interval', type=int, help='Interval in seconds')
              parser.add_argument('--output-dir', default='.', help='Output directory')
              parser.add_argument('--quality', choices=['high', 'highest'], default='highest')
              parser.add_argument('--pdf-dpi', type=int, default=600)
              args = parser.parse_args()
              success = process_video(args.url, args.interval, args.output_dir, args.quality, args.pdf_dpi)
              sys.exit(0 if success else 1)
          EOF
          chmod +x youtube_screenshots.py

      - name: Process video
        run: |
          mkdir -p output
          echo "Processing video from: ${{ matrix.source }}"
          echo "URL: ${{ matrix.url }}"
          echo "Interval: ${{ matrix.interval }} seconds"
          python youtube_screenshots.py "${{ matrix.url }}" "${{ matrix.interval }}" --output-dir output --quality highest --pdf-dpi 600

      - name: Upload video artifacts
        uses: actions/upload-artifact@v4
        with:
          name: video-${{ strategy.job-index }}
          path: output/
          retention-days: 90

  collect-results:
    needs: process-video
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: downloaded-artifacts

      - name: Merge all outputs
        run: |
          mkdir -p output
          
          # Merge all video outputs
          if [ -d "downloaded-artifacts" ]; then
            for artifact_dir in downloaded-artifacts/video-*; do
              if [ -d "$artifact_dir" ]; then
                echo "Merging: $artifact_dir"
                cp -r "$artifact_dir"/* output/ 2>/dev/null || true
              fi
            done
          fi
          
          echo "Merged output structure:"
          ls -la output/

      - name: Create index
        run: |
          CURRENT_DATE=$(date)
          cat > output/INDEX.md << 'ENDINDEX'
          # YouTube Screenshot Processing - Index
          
          ENDINDEX
          
          echo "**Generated:** $CURRENT_DATE" >> output/INDEX.md
          echo "**Workflow Run:** ${{ github.run_number }}" >> output/INDEX.md
          echo "" >> output/INDEX.md
          echo "---" >> output/INDEX.md
          echo "" >> output/INDEX.md
          echo "## Processed Videos" >> output/INDEX.md
          echo "" >> output/INDEX.md
          
          if find output -name "*_HD.pdf" -type f 2>/dev/null | grep -q .; then
            echo "| Video | PDF | Transcript | Screenshots |" >> output/INDEX.md
            echo "|-------|-----|------------|-------------|" >> output/INDEX.md
            for pdf in $(find output -name "*_HD.pdf" -type f | sort); do
              dir=$(dirname "$pdf")
              basename=$(basename "$dir")
              pdf_name=$(basename "$pdf")
              transcript="${dir}/${basename}_transcript.txt"
              screenshot_count=$(find "${dir}/images" -type f 2>/dev/null | wc -l)
              transcript_link="❌"
              if [ -f "$transcript" ]; then
                transcript_link="✅"
              fi
              echo "| $basename | [Download](./$basename/$pdf_name) | $transcript_link | $screenshot_count |" >> output/INDEX.md
            done
          else
            echo "*No videos processed.*" >> output/INDEX.md
          fi

      - name: Upload final PDFs
        uses: actions/upload-artifact@v4
        with:
          name: all-pdfs
          path: output/**/*_HD.pdf
          retention-days: 90

      - name: Upload final transcripts
        uses: actions/upload-artifact@v4
        with:
          name: all-transcripts
          path: output/**/*_transcript.txt
          retention-days: 90

      - name: Commit to repository
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add output/
          if git diff --staged --quiet; then
            echo "No changes"
          else
            git commit -m "YouTube screenshots - batch ${{ github.run_number }} [skip ci]"
            git push || echo "Push failed"
          fi

      - name: Summary
        run: |
          echo "# Processing Complete" >> $GITHUB_STEP_SUMMARY
          PDF_COUNT=$(find output -name "*_HD.pdf" -type f 2>/dev/null | wc -l)
          echo "- Total PDFs: $PDF_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- Check output/ directory or artifacts" >> $GITHUB_STEP_SUMMARY
