name: YouTube Screenshot Capture - All TXT Files

on:
  push:
    branches:
      - main
      - master
    paths:
      - '**/*.txt'  # Trigger on ANY .txt file change
      - '.github/workflows/youtube-screenshot.yml'
  workflow_dispatch:
    inputs:
      video_url:
        description: 'YouTube video URL (optional - for single video)'
        required: false
        type: string
      interval:
        description: 'Screenshot interval in seconds'
        required: false
        default: '5'
        type: string

jobs:
  process-videos:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install system dependencies
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y ffmpeg
          echo "‚úì FFmpeg installed"

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install yt-dlp Pillow numpy
          echo "‚úì Python dependencies installed"

      - name: Create processing script
        run: |
          cat > youtube_screenshots.py << 'EOF'
          #!/usr/bin/env python3
          """Enhanced YouTube Video Screenshot Capture Tool"""
          
          import os
          import sys
          import re
          import subprocess
          import argparse
          import tempfile
          import shutil
          from pathlib import Path
          import json
          import hashlib
          from PIL import Image
          from datetime import timedelta
          import textwrap
          
          def sanitize_filename(filename, max_length=100):
              invalid_chars = '<>:"/\\|?*'
              for char in invalid_chars:
                  filename = filename.replace(char, '')
              filename = filename.strip('. ')
              filename = filename.replace(' ', '_')
              if len(filename) > max_length:
                  filename = filename[:max_length]
              return filename
          
          def format_time(seconds):
              return str(timedelta(seconds=int(seconds)))
          
          def get_video_info(url):
              try:
                  cmd = ['yt-dlp', '--dump-json', '--no-playlist', url]
                  result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
                  info = json.loads(result.stdout)
                  subtitles_available = bool(info.get('subtitles', {})) or bool(info.get('automatic_captions', {}))
                  return {
                      'title': info.get('title', 'untitled'),
                      'duration': info.get('duration', 0),
                      'uploader': info.get('uploader', 'Unknown'),
                      'view_count': info.get('view_count', 0),
                      'subtitles_available': subtitles_available
                  }
              except Exception as e:
                  print(f"Error getting video info: {e}")
                  return None
          
          def download_video_and_transcript(url, video_path, transcript_path, force_hd=True):
              try:
                  format_options = [
                      'bestvideo[height>=1080][ext=mp4]+bestaudio[ext=m4a]/best[height>=1080][ext=mp4]',
                      'bestvideo[height>=720][ext=mp4]+bestaudio[ext=m4a]/best[height>=720][ext=mp4]',
                      'best[ext=mp4]/best'
                  ]
                  cmd = [
                      'yt-dlp', '-f', '/'.join(format_options), '--no-playlist',
                      '--merge-output-format', 'mp4', '-o', video_path,
                      '--write-auto-subs', '--write-subs', '--sub-lang', 'en',
                      '--convert-subs', 'srt', url
                  ]
                  print("Downloading video...")
                  subprocess.run(cmd, check=True)
                  video_dir = os.path.dirname(video_path)
                  video_base = os.path.splitext(os.path.basename(video_path))[0]
                  subtitle_patterns = [
                      f"{video_base}.en.srt", f"{video_base}.en.vtt",
                      f"{video_base}.srt", f"{video_base}.vtt"
                  ]
                  transcript_found = False
                  for pattern in subtitle_patterns:
                      potential_file = os.path.join(video_dir, pattern)
                      if os.path.exists(potential_file):
                          convert_srt_to_text(potential_file, transcript_path)
                          transcript_found = True
                          break
                  return True, transcript_found
              except Exception as e:
                  print(f"Error downloading video: {e}")
                  return False, False
          
          def convert_srt_to_text(srt_file, text_file):
              try:
                  with open(srt_file, 'r', encoding='utf-8', errors='ignore') as f:
                      lines = f.readlines()
                  transcript_lines = []
                  current_text = []
                  for line in lines:
                      line = line.strip()
                      if line and not line.isdigit() and '-->' not in line:
                          line = re.sub('<[^<]+?>', '', line)
                          current_text.append(line)
                      elif not line and current_text:
                          transcript_lines.append(' '.join(current_text))
                          current_text = []
                  if current_text:
                      transcript_lines.append(' '.join(current_text))
                  with open(text_file, 'w', encoding='utf-8') as f:
                      f.write("VIDEO TRANSCRIPT\n")
                      f.write("=" * 50 + "\n\n")
                      full_text = ' '.join(transcript_lines)
                      full_text = re.sub(r'\s+', ' ', full_text)
                      wrapped_text = textwrap.fill(full_text, width=80)
                      f.write(wrapped_text)
                  return True
              except Exception as e:
                  print(f"Error converting transcript: {e}")
                  return False
          
          def extract_high_quality_screenshots(video_path, output_dir, interval, title_prefix, quality='highest'):
              try:
                  os.makedirs(output_dir, exist_ok=True)
                  cmd = [
                      'ffprobe', '-v', 'error', '-show_entries', 'format=duration',
                      '-of', 'default=noprint_wrappers=1:nokey=1', video_path
                  ]
                  result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                  duration = float(result.stdout.strip())
                  print(f"Video duration: {duration:.1f} seconds")
                  print(f"Extracting screenshots every {interval} seconds...")
                  screenshots_taken = 0
                  current_time = 0
                  screenshot_files = []
                  while current_time <= duration:
                      time_str = f"{int(current_time):04d}s"
                      if quality == 'highest':
                          output_file = os.path.join(output_dir, f"{title_prefix}_{time_str}.png")
                          cmd = ['ffmpeg', '-ss', str(current_time), '-i', video_path,
                                 '-vframes', '1', '-vf', 'scale=iw:ih', '-y', output_file]
                      else:
                          output_file = os.path.join(output_dir, f"{title_prefix}_{time_str}.jpg")
                          cmd = ['ffmpeg', '-ss', str(current_time), '-i', video_path,
                                 '-vframes', '1', '-q:v', '1', '-y', output_file]
                      subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
                      screenshots_taken += 1
                      screenshot_files.append(output_file)
                      progress = (current_time / duration) * 100
                      print(f"  [{progress:5.1f}%] Screenshot at {format_time(current_time)}")
                      current_time += interval
                  return screenshots_taken, screenshot_files
              except Exception as e:
                  print(f"Error extracting screenshots: {e}")
                  return 0, []
          
          def get_image_hash(image_path):
              try:
                  with open(image_path, 'rb') as f:
                      return hashlib.sha256(f.read()).hexdigest()
              except Exception:
                  return None
          
          def remove_duplicate_screenshots(screenshot_files):
              if len(screenshot_files) <= 1:
                  return 0
              print("\nRemoving duplicate screenshots...")
              duplicates_removed = 0
              files_to_remove = set()
              processed_hashes = {}
              for current_file in screenshot_files:
                  if current_file in files_to_remove or not os.path.exists(current_file):
                      continue
                  current_hash = get_image_hash(current_file)
                  if not current_hash:
                      continue
                  if current_hash in processed_hashes:
                      files_to_remove.add(current_file)
                  else:
                      processed_hashes[current_hash] = current_file
              for file_path in files_to_remove:
                  try:
                      os.remove(file_path)
                      duplicates_removed += 1
                  except Exception:
                      pass
              return duplicates_removed
          
          def create_hd_pdf(images_dir, pdf_path, dpi=600):
              try:
                  print(f"\nCreating PDF (DPI: {dpi})...")
                  image_files = sorted(list(Path(images_dir).glob('*.jpg')) + list(Path(images_dir).glob('*.png')))
                  if not image_files:
                      return False
                  pdf_images = []
                  for img_path in image_files:
                      try:
                          img = Image.open(img_path)
                          if img.mode != 'RGB':
                              img = img.convert('RGB')
                          max_dimension = 3000
                          if img.width > max_dimension or img.height > max_dimension:
                              img.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)
                          pdf_images.append(img)
                      except Exception:
                          pass
                  if pdf_images:
                      pdf_images[0].save(
                          pdf_path, "PDF", resolution=dpi, quality=95, optimize=True,
                          save_all=True, append_images=pdf_images[1:] if len(pdf_images) > 1 else []
                      )
                      file_size = os.path.getsize(pdf_path) / (1024 * 1024)
                      print(f"  ‚úì PDF created: {file_size:.2f} MB")
                      return True
                  return False
              except Exception as e:
                  print(f"Error creating PDF: {e}")
                  return False
          
          def process_video(url, interval, output_dir='.', quality='highest',
                           pdf_dpi=600, keep_video=False, no_transcript=False, no_pdf=False):
              print(f"Fetching video information...")
              video_info = get_video_info(url)
              if not video_info:
                  return False
              safe_title = sanitize_filename(video_info['title'])
              print(f"\n{'='*60}")
              print(f"Video: {video_info['title']}")
              print(f"Duration: {format_time(video_info['duration'])}")
              print(f"Uploader: {video_info['uploader']}")
              print(f"{'='*60}\n")
              base_dir = Path(output_dir)
              video_dir = base_dir / safe_title
              images_dir = video_dir / 'images'
              images_dir.mkdir(parents=True, exist_ok=True)
              with tempfile.TemporaryDirectory() as temp_dir:
                  video_path = os.path.join(temp_dir, 'video.mp4')
                  transcript_path = video_dir / f"{safe_title}_transcript.txt"
                  video_success, transcript_success = download_video_and_transcript(
                      url, video_path, transcript_path if not no_transcript else None
                  )
                  if not video_success:
                      print("Error: Failed to download video")
                      return False
                  print(f"\n‚úì Video downloaded successfully!")
                  screenshot_count, screenshot_files = extract_high_quality_screenshots(
                      video_path, images_dir, interval, safe_title, quality
                  )
                  if screenshot_count > 0:
                      print(f"\n‚úì Extracted {screenshot_count} screenshots")
                      duplicates = remove_duplicate_screenshots(screenshot_files)
                      if duplicates > 0:
                          print(f"‚úì Removed {duplicates} duplicates")
                          print(f"  Final: {screenshot_count - duplicates} unique screenshots")
                      if not no_pdf:
                          pdf_path = video_dir / f"{safe_title}_HD.pdf"
                          create_hd_pdf(images_dir, pdf_path, pdf_dpi)
                      if keep_video:
                          final_video_path = video_dir / f"{safe_title}.mp4"
                          shutil.copy2(video_path, final_video_path)
                      print(f"\n{'='*60}")
                      print(f"‚úÖ COMPLETED SUCCESSFULLY!")
                      print(f"{'='*60}")
                      print(f"üìÅ Output: {video_dir.absolute()}")
                      print(f"üñºÔ∏è  Screenshots: {len(screenshot_files) - duplicates}")
                      if not no_pdf:
                          print(f"üìÑ PDF: {pdf_path.name}")
                      if transcript_success:
                          print(f"üìù Transcript: {transcript_path.name}")
                      print(f"{'='*60}\n")
                      return True
                  else:
                      print("Error: No screenshots extracted")
                      return False
          
          if __name__ == "__main__":
              parser = argparse.ArgumentParser(description='YouTube Screenshot Tool')
              parser.add_argument('url', help='YouTube video URL')
              parser.add_argument('interval', type=int, help='Interval in seconds')
              parser.add_argument('--output-dir', default='.', help='Output directory')
              parser.add_argument('--quality', choices=['high', 'highest'], default='highest')
              parser.add_argument('--pdf-dpi', type=int, default=600)
              parser.add_argument('--keep-video', action='store_true')
              parser.add_argument('--no-transcript', action='store_true')
              parser.add_argument('--no-pdf', action='store_true')
              args = parser.parse_args()
              success = process_video(
                  args.url, args.interval, args.output_dir,
                  args.quality, args.pdf_dpi, args.keep_video,
                  args.no_transcript, args.no_pdf
              )
              sys.exit(0 if success else 1)
          EOF
          
          chmod +x youtube_screenshots.py
          echo "‚úì Processing script created"

      - name: Find all .txt files with video URLs
        id: find_files
        run: |
          echo "Searching for .txt files with YouTube URLs..."
          
          # Find all .txt files (excluding output directory and hidden files)
          find . -type f -name "*.txt" \
            ! -path "./output/*" \
            ! -path "./.git/*" \
            ! -path "./.*" \
            > found_files.txt
          
          # Display found files
          echo "Found .txt files:"
          cat found_files.txt
          
          # Count files
          FILE_COUNT=$(wc -l < found_files.txt)
          echo "total_files=$FILE_COUNT" >> $GITHUB_OUTPUT
          
          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "‚ö†Ô∏è  No .txt files found"
          else
            echo "‚úì Found $FILE_COUNT .txt file(s)"
          fi

      - name: Process all .txt files with video URLs
        if: steps.find_files.outputs.total_files > 0
        run: |
          mkdir -p output
          
          echo "========================================="
          echo "Processing all .txt files"
          echo "========================================="
          
          while IFS= read -r txt_file; do
            echo ""
            echo "üìÑ Processing file: $txt_file"
            echo "-----------------------------------------"
            
            # Skip empty files
            if [ ! -s "$txt_file" ]; then
              echo "‚è≠Ô∏è  Skipping empty file"
              continue
            fi
            
            # Process each line in the .txt file
            while IFS= read -r line || [ -n "$line" ]; do
              # Skip empty lines and comments
              if [[ -z "$line" ]] || [[ "$line" =~ ^[[:space:]]*# ]]; then
                continue
              fi
              
              # Extract URL and interval (format: URL,interval or just URL)
              URL=$(echo "$line" | cut -d',' -f1 | xargs)
              INTERVAL=$(echo "$line" | cut -d',' -f2 | xargs)
              
              # Validate URL
              if [[ ! "$URL" =~ ^https?:// ]]; then
                echo "‚è≠Ô∏è  Skipping invalid URL: $URL"
                continue
              fi
              
              # Default interval if not specified
              if [ -z "$INTERVAL" ] || [ "$INTERVAL" = "$URL" ]; then
                INTERVAL=5
              fi
              
              echo ""
              echo "üé¨ Processing video:"
              echo "   URL: $URL"
              echo "   Interval: $INTERVAL seconds"
              echo "   Source: $txt_file"
              echo ""
              
              # Process the video
              python youtube_screenshots.py \
                "$URL" \
                "$INTERVAL" \
                --output-dir output \
                --quality highest \
                --pdf-dpi 600 || echo "‚ö†Ô∏è  Failed to process $URL"
              
            done < "$txt_file"
            
          done < found_files.txt
          
          echo ""
          echo "========================================="
          echo "‚úÖ All .txt files processed"
          echo "========================================="

      - name: Process single video (manual trigger)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.video_url != ''
        run: |
          mkdir -p output
          python youtube_screenshots.py \
            "${{ github.event.inputs.video_url }}" \
            "${{ github.event.inputs.interval }}" \
            --output-dir output \
            --quality highest \
            --pdf-dpi 600

      - name: Create summary and index
        run: |
          mkdir -p output
          
          # Create index of all processed videos
          cat > output/INDEX.md << 'INDEXEOF'
          # üìπ YouTube Screenshot Processing - Index
          
          **Generated:** $(date)
          **Workflow Run:** ${{ github.run_number }}
          **Commit:** ${{ github.sha }}
          
          ---
          
          ## üìÅ Processed Videos
          
          INDEXEOF
          
          # Find all PDFs and create index
          if find output -name "*.pdf" -type f | grep -q .; then
            echo "" >> output/INDEX.md
            echo "| Video | PDF | Transcript | Screenshots |" >> output/INDEX.md
            echo "|-------|-----|------------|-------------|" >> output/INDEX.md
            
            for pdf in $(find output -name "*_HD.pdf" -type f | sort); do
              dir=$(dirname "$pdf")
              basename=$(basename "$dir")
              pdf_name=$(basename "$pdf")
              transcript="${dir}/${basename}_transcript.txt"
              screenshot_count=$(find "${dir}/images" -name "*.png" -o -name "*.jpg" 2>/dev/null | wc -l)
              
              transcript_link="‚ùå"
              if [ -f "$transcript" ]; then
                transcript_link="[üìù View]($(realpath --relative-to=output "$transcript"))"
              fi
              
              echo "| $basename | [üìÑ Download]($(realpath --relative-to=output "$pdf")) | $transcript_link | $screenshot_count |" >> output/INDEX.md
            done
          else
            echo "" >> output/INDEX.md
            echo "*No videos processed yet.*" >> output/INDEX.md
          fi
          
          # Add structure info
          cat >> output/INDEX.md << 'STRUCTEOF'
          
          ---
          
          ## üìÇ Directory Structure
```
          output/
          ‚îú‚îÄ‚îÄ INDEX.md                    (This file)
          ‚îú‚îÄ‚îÄ [Video_Title_1]/
          ‚îÇ   ‚îú‚îÄ‚îÄ [Video_Title_1]_HD.pdf          (High-quality PDF)
          ‚îÇ   ‚îú‚îÄ‚îÄ [Video_Title_1]_transcript.txt  (Video transcript)
          ‚îÇ   ‚îî‚îÄ‚îÄ images/
          ‚îÇ       ‚îú‚îÄ‚îÄ [Video_Title_1]_0000s.png
          ‚îÇ       ‚îú‚îÄ‚îÄ [Video_Title_1]_0005s.png
          ‚îÇ       ‚îî‚îÄ‚îÄ ...
          ‚îî‚îÄ‚îÄ [Video_Title_2]/
              ‚îî‚îÄ‚îÄ ...
```
          
          ---
          
          ## üìù How to Add More Videos
          
          1. Create or edit any `.txt` file in the repository
          2. Add YouTube URLs (one per line):
```
             https://youtu.be/VIDEO_ID1,5
             https://youtu.be/VIDEO_ID2,10
```
          3. Commit and push - the workflow will automatically process them!
          
          ## üì• Download Options
          
          - **Individual Files**: Click the PDF links above
          - **Complete Archive**: Download from GitHub Actions artifacts
          - **Clone Repository**: `git clone <repo-url>` to get all files
          
          STRUCTEOF
          
          echo "‚úì Index created at output/INDEX.md"

      - name: Upload PDFs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pdfs-${{ github.run_number }}
          path: output/**/*_HD.pdf
          retention-days: 90
          if-no-files-found: warn

      - name: Upload transcripts as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: transcripts-${{ github.run_number }}
          path: output/**/*_transcript.txt
          retention-days: 90
          if-no-files-found: warn

      - name: Upload complete output as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: complete-output-${{ github.run_number }}
          path: output/
          retention-days: 90
          if-no-files-found: warn

      - name: Commit results to repository
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Add all generated files
          git add output/ || true
          
          # Check if there are changes
          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è  No new files to commit"
          else
            # Create detailed commit message
            FILES_ADDED=$(git diff --staged --name-only | wc -l)
            git commit -m "üé¨ Auto-generated: YouTube screenshots and PDFs

          - Files added: $FILES_ADDED
          - Workflow run: ${{ github.run_number }}
          - Commit: ${{ github.sha }}
          
          [skip ci]"
            
            echo "‚úÖ Committed $FILES_ADDED files"
            
            # Push changes
            git push || {
              echo "‚ö†Ô∏è  Push failed. You may need to:"
              echo "   1. Enable 'Read and write permissions' in Settings > Actions > General"
              echo "   2. Or manually pull and push the changes"
            }
          fi

      - name: Create job summary
        run: |
          echo "# üé¨ YouTube Screenshot Processing Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## üìä Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          PDF_COUNT=$(find output -name "*_HD.pdf" -type f 2>/dev/null | wc -l)
          TXT_COUNT=$(find output -name "*_transcript.txt" -type f 2>/dev/null | wc -l)
          IMG_COUNT=$(find output -name "*.png" -o -name "*.jpg" -type f 2>/dev/null | wc -l)
          
          echo "- üìÑ PDFs generated: **$PDF_COUNT**" >> $GITHUB_STEP_SUMMARY
          echo "- üìù Transcripts: **$TXT_COUNT**" >> $GITHUB_STEP_SUMMARY
          echo "- üñºÔ∏è Screenshots: **$IMG_COUNT**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## üì• Download" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Files are available in three ways:" >> $GITHUB_STEP_SUMMARY
          echo "1. **Artifacts** (scroll down) - Separate PDFs and transcripts" >> $GITHUB_STEP_SUMMARY
          echo "2. **Repository** - Files committed to \`output/\` directory" >> $GITHUB_STEP_SUMMARY
          echo "3. **View Index** - Check \`output/INDEX.md\` for direct links" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$PDF_COUNT" -gt 0 ]; then
            echo "## üìã Processed Videos" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            for pdf in $(find output -name "*_HD.pdf" -type f | sort); do
              dir=$(dirname "$pdf")
              basename=$(basename "$dir")
              size=$(du -h "$pdf" | cut -f1)
              echo "- **$basename** ($size)" >> $GITHUB_STEP_SUMMARY
            done
          fi
